#!/usr/bin/env python3
"""æ£€ç´¢å™¨å’Œæ£€ç´¢Agent - åŸºäºBGE-M3å’ŒCross-Encoderçš„æ³•å¾‹æ¡æ–‡æ£€ç´¢ï¼Œæ”¯æŒå¤šè·¯æŸ¥è¯¢"""

import os
import json
import warnings
import re
import math
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
import pickle
import jieba


class LocalRetriever:
    """Local vector retriever using BGE-M3 and FAISS"""
    
    def __init__(
        self,
        embedding_model_name: str = "BAAI/bge-m3",
        cross_encoder_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        index_path: str = "data/vectors.index",
        texts_path: str = "data/texts.pkl",
        metadatas_path: str = "data/metadatas.pkl"
    ):
        # è®¾ç½®æœ¬åœ°ç¼“å­˜ç›®å½•
        cache_dir = Path.home() / ".cache" / "sentence_transformers"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¦ç”¨åœ¨çº¿ä¸‹è½½è­¦å‘Š
        os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
        os.environ['HF_HUB_OFFLINE'] = '1'  # ä¼˜å…ˆç¦»çº¿æ¨¡å¼
        
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        self.embedding_model = self._load_model_with_cache(
            embedding_model_name, 
            cache_dir / embedding_model_name.replace('/', '--')
        )
        self.cross_encoder = self._load_model_with_cache(
            cross_encoder_name, 
            cache_dir / cross_encoder_name.replace('/', '--')
        )
        
        # å¦‚æœæœ¬åœ°æ²¡æœ‰ç¼“å­˜ï¼Œå…è®¸åœ¨çº¿ä¸‹è½½
        if self.embedding_model is None:
            warnings.warn(f"æœ¬åœ°æœªæ‰¾åˆ° {embedding_model_name}ï¼Œå°†åœ¨çº¿ä¸‹è½½...")
            os.environ.pop('HF_HUB_OFFLINE', None)
            self.embedding_model = SentenceTransformer(embedding_model_name)
            
        if self.cross_encoder is None:
            warnings.warn(f"æœ¬åœ°æœªæ‰¾åˆ° {cross_encoder_name}ï¼Œå°†åœ¨çº¿ä¸‹è½½...")
            os.environ.pop('HF_HUB_OFFLINE', None)
            self.cross_encoder = CrossEncoder(cross_encoder_name)
        
        self.index_path = index_path
        self.texts_path = texts_path
        self.metadatas_path = metadatas_path
        
        # åŠ è½½ç´¢å¼•å’Œæ•°æ®
        self.index = None
        self.texts = []
        self.metadatas = []
        self._load_or_create_index()
        
        # å…³é”®è¯æ£€ç´¢ç›¸å…³
        self.bm25_index = None
        self.term_frequencies = None
        self.doc_frequencies = None
        self.avg_doc_length = 0
        self._build_keyword_index()
    
    def _load_model_with_cache(self, model_name: str, cache_path: Path):
        """
        å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹
        
        Args:
            model_name: HuggingFaceæ¨¡å‹åç§°
            cache_path: æœ¬åœ°ç¼“å­˜è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ¨¡å‹æˆ–Noneï¼ˆå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼‰
        """
        try:
            if cache_path.exists() and any(cache_path.iterdir()):
                print(f"âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {cache_path}")
                if 'cross-encoder' in str(model_name):
                    return CrossEncoder(str(cache_path))
                else:
                    return SentenceTransformer(str(cache_path))
            else:
                print(f"âš ï¸ æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°: {cache_path}")
                return None
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None

    def _ensure_data_dir(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
    
    def _load_or_create_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•"""
        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
            with open(self.texts_path, 'rb') as f:
                self.texts = pickle.load(f)
            with open(self.metadatas_path, 'rb') as f:
                self.metadatas = pickle.load(f)
            print(f"Loaded existing index with {len(self.texts)} documents")
        else:
            print("No existing index found, will create new one")
    
    def add_documents(self, texts: List[str], metadatas: List[Dict[str, Any]] = None):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        if metadatas is None:
            metadatas = [{}] * len(texts)
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç´¢å¼•
        if self.index is None:
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        self.index.add(embeddings.astype('float32'))
        
        # ä¿å­˜æ–‡æœ¬å’Œå…ƒæ•°æ®
        self.texts.extend(texts)
        self.metadatas.extend(metadatas)
        
        # é‡å»ºå…³é”®è¯ç´¢å¼•
        self._build_keyword_index()
        
        # ä¿å­˜ç´¢å¼•å’Œæ•°æ®
        faiss.write_index(self.index, self.index_path)
        with open(self.texts_path, 'wb') as f:
            pickle.dump(self.texts, f)
        with open(self.metadatas_path, 'wb') as f:
            pickle.dump(self.metadatas, f)
        
        print(f"Added {len(texts)} documents to index")
    
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None or len(self.texts) == 0:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode([query], normalize_embeddings=True)
        query_vector = np.array(query_vector).astype('float32')
        
        # FAISSæœç´¢
        scores, indices = self.index.search(query_vector, top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.texts):
                results.append({
                    'text': self.texts[idx],
                    'metadata': self.metadatas[idx],
                    'score': float(score),
                    'index': int(idx)  # æ·»åŠ ç´¢å¼•å­—æ®µä»¥ä¾¿èåˆæ—¶åŒ¹é…
                })
        
        return results
    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_n: int = 5) -> List[Dict[str, Any]]:
        """æ£€ç´¢å™¨å’Œæ£€ç´¢Agent - åŸºäºBGE-M3å’ŒCross-Encoderçš„æ³•å¾‹æ¡æ–‡æ£€ç´¢"""
        if not documents:
            return []
        
        # æ„å»ºè¾“å…¥å¯¹
        pairs = [[query, doc['text']] for doc in documents]
        
        # è®¡ç®—é‡æ’åºåˆ†æ•°
        scores = self.cross_encoder.predict(pairs)
        
        # æ·»åŠ åˆ°æ–‡æ¡£å¹¶æ’åº
        for doc, score in zip(documents, scores):
            doc['rerank_score'] = float(score)
        
        # æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åˆ—
        reranked = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
        return reranked[:top_n]
    
    def retrieve_and_rerank(self, query: str, top_k: int = 10, top_n: int = 5) -> List[Dict[str, Any]]:
        """å®Œæ•´æ£€ç´¢æµç¨‹ï¼šæœç´¢+é‡æ’åº"""
        # æ­¥éª¤1ï¼šå‘é‡æœç´¢
        search_results = self.search(query, top_k)
        
        # æ­¥éª¤2ï¼šé‡æ’åº
        reranked_results = self.rerank(query, search_results, top_n)
        
        return reranked_results
    
    def _build_keyword_index(self):
        """æ„å»ºBM25å…³é”®è¯ç´¢å¼•"""
        if not self.texts:
            return
            
        # å®šä¹‰åœç”¨è¯
        self.stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'ä»¬', 'æ¥', 'è¿‡', 'æ—¶', 'å¾ˆ', 'è¿˜', 'ä¸ª', 'ä¸­', 'å¯ä»¥', 'è¿™ä¸ª', 'ç°åœ¨', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å› ä¸º', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'æˆ–è€…', 'ä»¥åŠ', 'ç­‰ç­‰', 'æ¯”å¦‚', 'ä¾‹å¦‚'
        }
        
        # åˆ†è¯å¹¶è®¡ç®—è¯é¢‘
        self.term_frequencies = []
        self.doc_frequencies = defaultdict(int)
        total_length = 0
        
        for text in self.texts:
            # ä¸­æ–‡åˆ†è¯ï¼Œå¯ç”¨ç²¾ç¡®æ¨¡å¼
            tokens = list(jieba.cut(text, cut_all=False))
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯ï¼Œä¿ç•™æ³•å¾‹æœ¯è¯­
            filtered_tokens = []
            for token in tokens:
                token = token.strip()
                if len(token) >= 1 and token not in self.stopwords:
                    # ä¿ç•™æ•°å­—ã€æ³•æ¡ç¼–å·ã€æ³•å¾‹æœ¯è¯­
                    if (token.isdigit() or 
                        re.match(r'ç¬¬\d+æ¡', token) or 
                        re.match(r'ç¬¬\d+ç« ', token) or
                        re.match(r'ã€Š.*ã€‹', token) or
                        len(token) >= 2):
                        filtered_tokens.append(token)
            
            doc_tf = Counter(filtered_tokens)
            self.term_frequencies.append(doc_tf)
            total_length += len(filtered_tokens)
            
            # è®¡ç®—æ–‡æ¡£é¢‘ç‡
            for term in set(filtered_tokens):
                self.doc_frequencies[term] += 1
        
        self.avg_doc_length = total_length / len(self.texts) if self.texts else 0
        
    def keyword_search(self, query: str, top_k: int = 10, k1: float = 1.5, b: float = 0.75) -> List[Dict[str, Any]]:
        """
        BM25å…³é”®è¯æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            k1: BM25å‚æ•°k1
            b: BM25å‚æ•°b
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.texts or not self.term_frequencies:
            return []
            
        # å¯¹æŸ¥è¯¢è¿›è¡Œç›¸åŒçš„åˆ†è¯å’Œè¿‡æ»¤å¤„ç†
        query_tokens = list(jieba.cut(query, cut_all=False))
        filtered_query_tokens = []
        for token in query_tokens:
            token = token.strip()
            if len(token) >= 1 and token not in self.stopwords:
                if (token.isdigit() or 
                    re.match(r'ç¬¬\d+æ¡', token) or 
                    re.match(r'ç¬¬\d+ç« ', token) or
                    re.match(r'ã€Š.*ã€‹', token) or
                    len(token) >= 2):
                    filtered_query_tokens.append(token)
        
        if not filtered_query_tokens:
            return []
            
        scores = []
        
        for i, doc_tf in enumerate(self.term_frequencies):
            score = 0
            doc_length = sum(doc_tf.values())
            
            for term in filtered_query_tokens:
                if term in doc_tf:
                    tf = doc_tf[term]
                    df = self.doc_frequencies[term]
                    idf = math.log((len(self.texts) - df + 0.5) / (df + 0.5))
                    
                    # BM25å…¬å¼
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * doc_length / self.avg_doc_length)
                    score += idf * numerator / denominator
            
            scores.append((i, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºç»“æœ
        results = []
        for doc_idx, score in scores[:top_k]:
            if score > 0:  # åªè¿”å›æœ‰åŒ¹é…çš„ç»“æœ
                result = {
                    'text': self.texts[doc_idx],
                    'metadata': self.metadatas[doc_idx] if doc_idx < len(self.metadatas) else {},
                    'score': float(score),
                    'index': doc_idx,
                    'search_type': 'keyword'
                }
                results.append(result)
        
        return results
    
    def hybrid_search(self, query: str, top_k: int = 10, vector_weight: float = 0.6, keyword_weight: float = 0.4) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢ + ç»“æœèåˆ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            
        Returns:
            èåˆåçš„æœç´¢ç»“æœ
        """
        print(f"ğŸ”„ [æ··åˆæœç´¢] æ‰§è¡Œå¹¶è¡Œæœç´¢ - å‘é‡æƒé‡:{vector_weight:.1f}, å…³é”®è¯æƒé‡:{keyword_weight:.1f}")
        
        # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æœç´¢
        vector_results = self.search(query, top_k * 2)  # è·å–æ›´å¤šå€™é€‰
        keyword_results = self.keyword_search(query, top_k * 2)
        
        print(f"ğŸ“Š [æœç´¢ç»“æœ] å‘é‡æœç´¢: {len(vector_results)}ä¸ª, å…³é”®è¯æœç´¢: {len(keyword_results)}ä¸ª")
        
        # ä½¿ç”¨RRF (Reciprocal Rank Fusion) èåˆç»“æœ
        fused_results = self._fuse_results(vector_results, keyword_results, vector_weight, keyword_weight, top_k)
        
        print(f"ğŸ¯ [èåˆå®Œæˆ] æœ€ç»ˆè¿”å›: {len(fused_results)}ä¸ªç»“æœ")
        return fused_results
    
    def _fuse_results(self, vector_results: List[Dict], keyword_results: List[Dict], 
                     vector_weight: float, keyword_weight: float, top_k: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨RRFç®—æ³•èåˆå¤šè·¯æœç´¢ç»“æœ
        
        Args:
            vector_results: å‘é‡æœç´¢ç»“æœ
            keyword_results: å…³é”®è¯æœç´¢ç»“æœ
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        k = 20  # é™ä½RRFå‚æ•°ï¼Œå¢åŠ æ’åé å‰ç»“æœçš„æƒé‡
        doc_scores = defaultdict(float)
        doc_info = {}
        
        print(f"ğŸ”€ [RRFèåˆ] å¼€å§‹èåˆç»“æœ - RRFå‚æ•°k={k}")
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = vector_weight + keyword_weight
        if total_weight > 0:
            vector_weight = vector_weight / total_weight
            keyword_weight = keyword_weight / total_weight
        
        print(f"âš–ï¸ [æƒé‡å½’ä¸€åŒ–] å‘é‡:{vector_weight:.3f}, å…³é”®è¯:{keyword_weight:.3f}")
        
        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        for rank, result in enumerate(vector_results):
            doc_id = result.get('index', rank)
            rrf_score = vector_weight / (k + rank + 1)
            doc_scores[doc_id] += rrf_score
            
            if doc_id not in doc_info:
                doc_info[doc_id] = {
                    'text': result['text'],
                    'metadata': result.get('metadata', {}),
                    'vector_score': result.get('score', 0),
                    'keyword_score': 0,
                    'index': doc_id
                }
        
        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for rank, result in enumerate(keyword_results):
            doc_id = result.get('index', result.get('text', rank))
            # å¯¹å…³é”®è¯æœç´¢ç»™äºˆæ›´é«˜çš„åŸºç¡€æƒé‡
            base_score = keyword_weight * 2  # å¢åŠ å…³é”®è¯æœç´¢çš„åŸºç¡€æƒé‡
            rrf_score = base_score / (k + rank + 1)
            doc_scores[doc_id] += rrf_score
            
            if doc_id not in doc_info:
                doc_info[doc_id] = {
                    'text': result['text'],
                    'metadata': result.get('metadata', {}),
                    'vector_score': 0,
                    'keyword_score': result.get('score', 0),
                    'index': doc_id
                }
            else:
                doc_info[doc_id]['keyword_score'] = result.get('score', 0)
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        print(f"ğŸ“ˆ [èåˆç»Ÿè®¡] å¤„ç†äº†{len(doc_info)}ä¸ªå”¯ä¸€æ–‡æ¡£")
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_results = []
        for i, (doc_id, fused_score) in enumerate(sorted_docs[:top_k]):
            info = doc_info[doc_id]
            result = {
                'text': info['text'],
                'metadata': info['metadata'],
                'score': fused_score,
                'vector_score': info['vector_score'],
                'keyword_score': info['keyword_score'],
                'index': info['index'],
                'search_type': 'hybrid'
            }
            final_results.append(result)
            
            # æ˜¾ç¤ºå‰3ä¸ªç»“æœçš„èåˆè¯¦æƒ…
            if i < 3:
                print(f"ğŸ† [Top{i+1}] èåˆåˆ†æ•°:{fused_score:.4f} = å‘é‡:{info['vector_score']:.3f} + å…³é”®è¯:{info['keyword_score']:.3f}")
        
        return final_results
    
    def smart_search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½æœç´¢ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æœç´¢ç­–ç•¥
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ” [æ™ºèƒ½æœç´¢] å¼€å§‹åˆ†ææŸ¥è¯¢: '{query}'")
        
        # åˆ†ææŸ¥è¯¢ç±»å‹
        query_type = self._analyze_query_type(query)
        
        if query_type == 'exact':
            # ç²¾ç¡®æŸ¥è¯¢ï¼šä¼˜å…ˆå…³é”®è¯æœç´¢
            print(f"ğŸ“ [æŸ¥è¯¢ç±»å‹] ç²¾ç¡®æŸ¥è¯¢ - å…³é”®è¯æƒé‡70%, å‘é‡æƒé‡30%")
            return self.hybrid_search(query, top_k, vector_weight=0.3, keyword_weight=0.7)
        elif query_type == 'semantic':
            # è¯­ä¹‰æŸ¥è¯¢ï¼šä¼˜å…ˆå‘é‡æœç´¢
            print(f"ğŸ§  [æŸ¥è¯¢ç±»å‹] è¯­ä¹‰æŸ¥è¯¢ - å‘é‡æƒé‡80%, å…³é”®è¯æƒé‡20%")
            return self.hybrid_search(query, top_k, vector_weight=0.8, keyword_weight=0.2)
        else:
            # æ··åˆæŸ¥è¯¢ï¼šå¹³è¡¡æƒé‡
            print(f"âš–ï¸ [æŸ¥è¯¢ç±»å‹] æ··åˆæŸ¥è¯¢ - å‘é‡æƒé‡60%, å…³é”®è¯æƒé‡40%")
            return self.hybrid_search(query, top_k, vector_weight=0.6, keyword_weight=0.4)
    
    def _analyze_query_type(self, query: str) -> str:
        """
        åˆ†ææŸ¥è¯¢ç±»å‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æŸ¥è¯¢ç±»å‹ï¼š'exact', 'semantic', 'mixed'
        """
        exact_score = 0
        semantic_score = 0
        
        # ç²¾ç¡®æŸ¥è¯¢ç‰¹å¾
        exact_patterns = [
            (r'ç¬¬\d+æ¡', 3),  # ç¬¬XXæ¡ - é«˜æƒé‡
            (r'ç¬¬\d+ç« ', 2),  # ç¬¬XXç« 
            (r'ã€Š[^ã€‹]+ã€‹', 2),  # æ³•å¾‹åç§°
            (r'\d+å¹´\d+æœˆ\d+æ—¥', 2),  # æ—¥æœŸ
            (r'[A-Z]{2,}', 1),  # è‹±æ–‡ç¼©å†™
            (r'è§„å®š|æ¡æ¬¾|æ³•æ¡|æ¡æ–‡', 2),  # æ³•å¾‹æœ¯è¯­
            (r'ç¬¬\d+æ¬¾|ç¬¬\d+é¡¹', 2),  # å…·ä½“æ¡æ¬¾é¡¹
        ]
        
        for pattern, weight in exact_patterns:
            if re.search(pattern, query):
                exact_score += weight
        
        # è¯­ä¹‰æŸ¥è¯¢ç‰¹å¾
        semantic_patterns = [
            (r'ä»€ä¹ˆæ˜¯|ä½•ä¸º|å®šä¹‰', 3),  # å®šä¹‰ç±»é—®é¢˜
            (r'å¦‚ä½•|æ€æ ·|æ€ä¹ˆ', 2),  # æ–¹æ³•ç±»é—®é¢˜
            (r'ä¸ºä»€ä¹ˆ|åŸå› |æ„ä¹‰', 2),  # åŸå› ç±»é—®é¢˜
            (r'æ¦‚å¿µ|åŸç†|æ€§è´¨', 2),  # æ¦‚å¿µç±»é—®é¢˜
            (r'åŒ…æ‹¬|æ¶‰åŠ|èŒƒå›´', 1),  # èŒƒå›´ç±»é—®é¢˜
            (r'åŒºåˆ«|ä¸åŒ|å·®å¼‚', 1),  # æ¯”è¾ƒç±»é—®é¢˜
        ]
        
        for pattern, weight in semantic_patterns:
            if re.search(pattern, query):
                semantic_score += weight
        
        # æ ¹æ®å¾—åˆ†å†³å®šæŸ¥è¯¢ç±»å‹
        if exact_score >= 3 and exact_score > semantic_score:
            return 'exact'
        elif semantic_score >= 2 and semantic_score > exact_score:
            return 'semantic'
        else:
            return 'mixed'


def create_local_retriever() -> LocalRetriever:
    """å·¥å‚å‡½æ•°åˆ›å»ºæœ¬åœ°æ£€ç´¢å™¨"""
    retriever = LocalRetriever()
    
    # å¦‚æœç´¢å¼•ä¸ºç©ºï¼ŒåŠ è½½æ³•å¾‹æ–‡æœ¬
    if retriever.index is None:
        from pathlib import Path
        law_file = Path(__file__).parent.parent / "chinese_law.txt"
        
        if law_file.exists():
            texts = []
            metadatas = []
            
            with open(law_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        texts.append(line)
                        
                        # æå–å…ƒæ•°æ®
                        metadata = {}
                        if 'ã€Š' in line and 'ã€‹' in line:
                            law_start = line.find('ã€Š')
                            law_end = line.find('ã€‹') + 1
                            metadata['law'] = line[law_start:law_end]
                        
                        if 'ç¬¬' in line and 'æ¡è§„å®š' in line:
                            article_start = line.find('ç¬¬')
                            article_end = line.find('æ¡è§„å®š') + 2
                            metadata['article'] = line[article_start:article_end]
                        
                        metadata['line_number'] = line_num
                        metadatas.append(metadata)
            
            if texts:
                retriever.add_documents(texts, metadatas)
    
    return retriever


# å…¨å±€æ£€ç´¢å™¨
retriever: Optional[LocalRetriever] = None


def get_retriever():
    """è·å–å…¨å±€æ£€ç´¢å™¨å®ä¾‹"""
    global retriever
    if retriever is None:
        retriever = create_local_retriever()
    return retriever


def execute_retrieval(query: str) -> str:
    """æ‰§è¡Œæ£€ç´¢å¹¶è¿”å›æ ¼å¼åŒ–ç»“æœ"""
    retriever = get_retriever()
    if retriever.index is None:
        return "âš ï¸ æ³•å¾‹æ–‡æœ¬ç´¢å¼•å°šæœªå»ºç«‹ï¼Œè¯·å…ˆè¿è¡Œ: python init_index.py"
    
    # æ‰§è¡Œæ™ºèƒ½å¤šè·¯æŸ¥è¯¢
    results = retriever.smart_search(query, top_k=10)
    
    # å¯¹ç»“æœè¿›è¡Œé‡æ’åºä»¥è¿›ä¸€æ­¥ä¼˜åŒ–
    if results:
        results = retriever.rerank(query, results, top_n=5)
    
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡"
    
    # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
    formatted_results = []
    for i, result in enumerate(results, 1):
        text = result['text']
        score = result.get('rerank_score', 0)
        metadata = result['metadata']
        
        law_name = metadata.get('law', 'æœªçŸ¥æ³•å¾‹')
        article = metadata.get('article', 'æœªçŸ¥æ¡æ¬¾')
        
        formatted_results.append(f"""ã€æ£€ç´¢ç»“æœ {i}ã€‘(ç›¸å…³æ€§: {score:.3f})
{law_name} {article}
{text}""")
    
    return "\n\n".join(formatted_results)


# æ£€ç´¢æ‰§è¡ŒAgent - å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
def create_retrieval_agent():
    """åˆ›å»ºæ£€ç´¢æ‰§è¡ŒAgent"""
    from google.adk.agents import LlmAgent
    from google.adk.models.lite_llm import LiteLlm
    from google.genai import types
    
    return LlmAgent(
        name="RetrievalAgent", 
        model=LiteLlm(model="deepseek/deepseek-chat"),
        instruction="""ä½ æ˜¯æ£€ç´¢æ‰§è¡Œä¸“å®¶ã€‚åŸºäºé‡å†™åçš„æŸ¥è¯¢æ‰§è¡Œæ³•å¾‹æ¡æ–‡æ£€ç´¢ã€‚

**é‡å†™åçš„æŸ¥è¯¢ï¼š**
{rewritten_query}

**æ‰§è¡Œæ­¥éª¤ï¼š**
1. è§£æé‡å†™åçš„æŸ¥è¯¢å†…å®¹
2. ä½¿ç”¨ä¸»æŸ¥è¯¢è¿›è¡Œæ£€ç´¢
3. å¦‚æœç»“æœä¸è¶³ï¼Œå°è¯•å¤‡é€‰æŸ¥è¯¢
4. åˆå¹¶å’Œå»é‡æ£€ç´¢ç»“æœ
5. æŒ‰ç›¸å…³æ€§æ’åº

**é‡è¦è§„åˆ™ï¼š**
- å¦‚æœæ£€ç´¢å·¥å…·è¿”å›"æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡"ï¼Œç›´æ¥è¾“å‡ºè¯¥ç»“æœ
- ä¸è¦åŸºäºç©ºç»“æœç”Ÿæˆä»»ä½•å†…å®¹

è¾“å‡ºæ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡å’Œç»Ÿè®¡ä¿¡æ¯ã€‚""",
        description="æ‰§è¡Œæ³•å¾‹æ¡æ–‡æ£€ç´¢",
        output_key="retrieval_results",
        tools=[execute_retrieval],
        generate_content_config=types.GenerateContentConfig(
            temperature=0.1,
            top_p=0.8,
            max_output_tokens=2048,
        )
    )

# æ‡’åŠ è½½æ£€ç´¢Agent
retrieval_agent = None

def get_retrieval_agent():
    """è·å–æ£€ç´¢Agentå®ä¾‹"""
    global retrieval_agent
    if retrieval_agent is None:
        retrieval_agent = create_retrieval_agent()
    return retrieval_agent